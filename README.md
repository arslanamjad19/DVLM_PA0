# DVLM_PA0

This report presents a comprehensive study of fundamental deep vision architectures, spanning Convolutional Neural Networks (CNNs), Vision Transformers (ViTs), Variational Autoencoders (VAEs), and Multimodal models. We first analyze the training dynamics of ResNet-152, demonstrating the critical role of residual connections in gradient flow and the hierarchical nature of learned features. Second, we explore the interpretability of Vision Transformers through attention map visualizations and assess their robustness to patch masking. Third, we investigate the phenomenon of posterior collapse in VAEs and propose a KL-annealing strategy to mitigate it. Finally, we examine the modality gap in CLIPâ€™s embedding space and demonstrate that geometric alignment via Procrustes analysis does not degrade zero-shot performance.
